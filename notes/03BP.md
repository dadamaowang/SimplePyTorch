# 03 BackPropagation





仍然是线性模型的例子：
$$
y = \omega*x
$$
不管模型是简单的还是复杂的（权重非常多），需要对权重不断更新

将网络看作一个 __图__   Computational Graph

在图上传播权重的更新



例子：一个简单的 two layer neural network:
$$
\hat{y}=W_2(W_1\cdot X+b_1)+b_2
$$
![BP01](C:\shelf\Projects\PytorchLeadIn\pics\BP01.PNG)

![BP02](C:\shelf\Projects\PytorchLeadIn\pics\BP02.PNG)

![BP03](C:\shelf\Projects\PytorchLeadIn\pics\BP03.PNG)

对这个神经网络的结构如图

对矩阵的运算推荐参考：__matrix cook book__

![BP04](C:\shelf\Projects\PytorchLeadIn\pics\BP04.PNG)

对每一层输出添加Nonlinear Function的原因：

观察$\hat{y}$ 的表达式可以知道，对它进行展开化简可以得到$\hat{y}=W\cdot X+b$,这样子两层就变回一层了没啥意义

对中间结果加非线性函数作变换，原式就无法展开了



求导：链式求导法则

_step 1_ 创建计算图 :

![BP05](C:\shelf\Projects\PytorchLeadIn\pics\BP05.PNG)

f计算出输出的z，loss函数计算偏导

_step 2_ 计算局部梯度->梯度下降算法



__在Pytorch中，所有数据都保存在tensor里，tensor类又两个重要成员：__

__data 和 grad__

分别保存权重本身的值，和，损失对权重的导数

定义tensor可以建立计算图

构建模型就是构建计算图



【code1】

```python
# 简单反向传播示例
import torch

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = torch.tensor([1.0])  # 记得中括号
w.requires_grad = True  # 设置为True表示对变量w自动求梯度
# 默认的Tensor不会计算关于w的梯度，这个要自己设定


# 模型
def forward(x):  # x是个tensor了，这里是tensor和tensor之间的乘法,得到的还是tensor
    return w * x
    # *运算符已经被重载了，要进行tensor与tensor之间的数乘
    # x被自动类型转换为tensor
    # 计算出节点的输入值也需要包含梯度


def loss(x, y):  # 损失
    y_pred = forward(x)
    return (y_pred - y) ** 2
# 看到这些运算就反应过来是构建计算图
# 从能画出计算图的角度考虑代码


for epoch in range(100):  # 训练100轮
    for x, y in zip(x_data, y_data):
        loss_value = loss(x, y)  # 前馈
        loss_value.backward()
        # 自动把计算图上的计算链路上所有要梯度的都计算出来
        # backward完毕后，把梯度存在需要的地方（变量自己里边）
        # 只要作完backward，这个计算图就没有了
        # 下次计算是新的计算图
        # 每进行一次反向传播，释放一次计算图，是一种灵活的方式
        print('\tgrad : ', x, y, w.grad.item())
        # w.grad获得梯度；
        # 张量里边的data和grad中，grad也是tensor
        # .item()为了只把数值拿出来当作标量

        # 所以要取出来data进行计算，（取data不会建立计算图）
        w.data = w.data - 0.01 * w.grad.data  # 因为这里只想作纯数值修改

        w.grad.data.zero_()
        # 一定显式清0，否则w中每次计算出的梯度会累加
        # 本例中由于就想一个样本一个样本地训，就不要累加
        # 事实上还有很多技巧是需要累加的，这就是w会自动累加的原因
        # 在不用那些技巧的时候，手动清零很必要

    print('progress : ', 'epoch:', epoch, 'loss_val:', loss_value.item())
    # notice: 打印一定是.item()取出来里边的标量



```

```
epoch:  97
	grad :  -7.152557373046875e-07
	grad :  -2.86102294921875e-06
	grad :  -5.7220458984375e-06
loss_val: 9.094947017729282e-13
epoch:  98
	grad :  -7.152557373046875e-07
	grad :  -2.86102294921875e-06
	grad :  -5.7220458984375e-06
loss_val: 9.094947017729282e-13
epoch:  99
	grad :  -7.152557373046875e-07
	grad :  -2.86102294921875e-06
	grad :  -5.7220458984375e-06
loss_val: 9.094947017729282e-13
```

【code2】

```python3
# 二层神经网络的反向传播示例
import torch

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w1 = torch.tensor([1.0])
w1.requires_grad = True
w2 = torch.tensor([2.0])
w2.requires_grad = True
b = torch.tensor([1.0])
b.requires_grad = True


def forward(x):
    return w1 * (x ** 2) + w2 * x + b


def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) ** 2


for epoch in range(100):
    print('epoch : ', epoch)
    for x, y in zip(x_data, y_data):
        loss_value = loss(x, y)
        loss_value.backward()
        print('\tgrad : ', 'w1: ', w1.grad.item(), 'w2:',  w2.grad.item(), 'b:', b.grad.item())
        w1.data = w1.data - 0.01 * w1.grad.data
        w2.data = w2.data - 0.01 * w2.grad.data
        b.data = b.data - 0.01 * b.grad.data
        w1.grad.data.zero_()
        w2.grad.data.zero_()
        b.grad.data.zero_()
    print('loss_val: ', loss_value.item())

```

```
epoch :  95
	grad :  w1:  0.19469690322875977 w2: 0.19469690322875977 b: 0.19469690322875977
	grad :  w1:  -0.8440914154052734 w2: -0.4220457077026367 b: -0.21102285385131836
	grad :  w1:  0.6485795974731445 w2: 0.21619319915771484 b: 0.07206439971923828
loss_val:  0.0012983194319531322
epoch :  96
	grad :  w1:  0.19382190704345703 w2: 0.19382190704345703 b: 0.19382190704345703
	grad :  w1:  -0.8460159301757812 w2: -0.4230079650878906 b: -0.2115039825439453
	grad :  w1:  0.6516609191894531 w2: 0.21722030639648438 b: 0.07240676879882812
loss_val:  0.0013106850674375892
epoch :  97
	grad :  w1:  0.1929769515991211 w2: 0.1929769515991211 b: 0.1929769515991211
	grad :  w1:  -0.8478355407714844 w2: -0.4239177703857422 b: -0.2119588851928711
	grad :  w1:  0.6546220779418945 w2: 0.21820735931396484 b: 0.07273578643798828
loss_val:  0.001322623691521585
epoch :  98
	grad :  w1:  0.19216156005859375 w2: 0.19216156005859375 b: 0.19216156005859375
	grad :  w1:  -0.8495674133300781 w2: -0.42478370666503906 b: -0.21239185333251953
	grad :  w1:  0.6574630737304688 w2: 0.21915435791015625 b: 0.07305145263671875
loss_val:  0.0013341286685317755

```



## 关于BP小结

BP基础：链式法则（复合函数求导可以使用乘法法则，又称链式法则）

神经网络简化：正向一遍loss,反向一遍求梯度，根据梯度更新权值











